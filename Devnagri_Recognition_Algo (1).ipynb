{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Devnagri_Recognition_Algo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQAGx2S34Imt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "from keras import models\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, LSTM, merge\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import csv\n",
        "from PIL import Image    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D,BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tqdm import tqdm\n",
        "import numpy as np # linear algebra\n",
        "from numpy import asarray\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import collections\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dropout\n",
        "## real time data augmentation ..\n",
        "\n",
        "Generation_train = ImageDataGenerator(\n",
        "\t\trotation_range = 5,          ## rotated image training\n",
        "\t\theight_shift_range = 0.1,   ## shifted image training\n",
        "\t\trescale = 1.0/255,\n",
        "\t\tshear_range = 0.2,\n",
        "\t\twidth_shift_range = 0.1,\n",
        "\t\tfill_mode = 'nearest')\n",
        "\n",
        "Generation_test = ImageDataGenerator(\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trotation_range=0.05\n",
        "                          rescale=1./255,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tfill_mode=\"nearest\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t     ### this is basically normalisation (ratio by which each pixel is multiplied)\n",
        "trainGenerator = Generation_train.flow_from_directory(\n",
        "\t\t\t\"D:\\DevanagariHandwrittenCharacterDataset\\Train\",    ## directory of foler having classes of training sets\n",
        "\t\t\tcolor_mode = \"grayscale\",\n",
        "\t\t\ttarget_size = (32,32),\n",
        "\t\t\tbatch_size = 32,\n",
        "\t\t\tclass_mode = \"categorical\")\n",
        "prev = \"\"\n",
        "labels = [\"ka\",\"kha\",\"ga\",\"gha\",\"kna\",\"cha\",\"chha\",\"ja\",\"jha\",\"yna\",\"t`a\",\"t`ha\",\"d`a\",\"d`ha\",\"adna\",\"ta\",\"tha\",\"da\",\"dha\",\"na\",\"pa\",\"pha\",\"ba\",\"bha\",\"ma\",\"yaw\",\"ra\",\"la\",\"waw\",\"sha\",\"shat\",\"sa\",\"ha\",\"aksha\",\"tra\",\"gya\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "count = 0;\n",
        "\n",
        "validation_generator = Generation_test.flow_from_directory(\n",
        "\t\t\t\"D:\\DevanagariHandwrittenCharacterDataset\\Test\",     ## directory of folder having tests set\n",
        "\t\t\ttarget_size=(32,32),\n",
        "\t\t\tbatch_size=32,\n",
        "\t\t\tcolor_mode = \"grayscale\",\n",
        "\t\t\tclass_mode= 'categorical')\n",
        "\t\t\t\n",
        "model = Sequential()\n",
        "\n",
        "##  introducing different layers + batch normalisation (gaussian curve)\n",
        "#Layer1----------------------------------------------------------\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), input_shape = (32,32,1), strides = 1, padding = \"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Activation(activations.relu))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(128, (3,3), strides = 1, padding = \"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Activation(activations.relu))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(512, (3,3), strides = 1,padding = \"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Activation(activations.relu))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(512, (3,3), strides = 1,padding = \"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Activation(activations.relu))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\", kernel_initializer = \"uniform\"))  # check for kernal initilaiser\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Activation(activations.relu))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512 ,activation = \"relu\", kernel_initializer = \"uniform\"))  #\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Activation(activations.relu))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(46, activation = \"softmax\", kernel_initializer = \"uniform\"))  #  check whether it needs ac\t\t\t\n",
        "\t\t\t\n",
        "model.compile(optimizer = \"adam\",\n",
        "\t\tloss = \"categorical_crossentropy\",\n",
        "\t\tmetrics = [\"accuracy\"])\n",
        "\t\t\n",
        "print(model.summary())\n",
        "\n",
        "res=model.fit_generator(\n",
        "\t\tGeneration_train,\n",
        "\t\tepochs = 200,       \n",
        "\t\tvalidation_data = validation_generator,\n",
        "\t\tvalidation_steps = 432,\n",
        "\t\tsteps_per_epoch = 200,\n",
        "\t\t)\n",
        "\n",
        "%matplotlib inline\n",
        "accu=res.history['accuracy']\n",
        "#val_acc=res.history['val_accuracy']\n",
        "loss=res.history['loss']\n",
        "#val_loss=res.history['val_loss']\n",
        "\n",
        "epochs=range(len(accu)) #No. of epochs\n",
        "\n",
        "# graph plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epochs,accu,'r',label='Training Accuracy')\n",
        "plt.plot(epochs,val_acc,'g',label='Testing Accuracy')  \n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "#Plot training and validation loss per epoch\n",
        "plt.plot(epochs,loss,'r',label='Training Loss')\n",
        "plt.plot(epochs,val_loss,'g',label='Testing Loss')   \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\t\t\n",
        "## saving the model --  we can do one thing is define two functions in a class..\n",
        "# one function will have model preparation..\n",
        "# the other will give results..\n",
        "\n",
        "# model.save(\"HindiModel2.h5\")\n",
        "\n",
        "''' [\"ka\",\"kha\",\"ga\",\"gha\",\"kna\",\"cha\",\"chha\",\"ja\",\"jha\",\"yna\",\"t`a\",\"t`ha\",\"d`a\",\"d`ha\",\"adna\",\"ta\",\"tha\",\"da\",\"dha\",\"na\",\"pa\",\"pha\",\"ba\",\"bha\",\"ma\",\"yaw\",\"ra\",\"la\",\"waw\",\"sha\",\"shat\",\"sa\",\"ha\",\"aksha\",\"tra\",\"gya\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "labels =['yna', 't`aa', 't`haa', 'd`aa', 'd`haa', 'a`dna', 'ta', 'tha', 'da', 'dha', 'ka', 'na', 'pa', 'pha', 'ba', 'bha', 'ma', 'yaw', 'ra', 'la', 'waw', 'kha', 'sha', 'shat', 'sa', 'ha', 'aksha', 'tra', 'gya', 'ga', 'gha', 'kna', 'cha', 'chha', 'ja', 'jha', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "'''\n",
        "labels = [u'\\u091E',u'\\u091F',u'\\u0920',u'\\u0921',u'\\u0922',u'\\u0923',u'\\u0924',u'\\u0925',u'\\u0926',u'\\u0927',u'\\u0915',u'\\u0928',u'\\u092A',u'\\u092B',u'\\u092c',u'\\u092d',u'\\u092e',u'\\u092f',u'\\u0930',u'\\u0932',u'\\u0935',u'\\u0916',u'\\u0936',u'\\u0937',u'\\u0938',u'\\u0939','ksha','tra','gya',u'\\u0917',u'\\u0918',u'\\u0919',u'\\u091a',u'\\u091b',u'\\u091c',u'\\u091d',u'\\u0966',u'\\u0967',u'\\u0968',u'\\u0969',u'\\u096a',u'\\u096b',u'\\u096c',u'\\u096d',u'\\u096e',u'\\u096f']\n",
        "#\n",
        "\n",
        "# loading the 'captcha.jpg' image..\n",
        "test_image = cv2.imread(\"205.png\")   # to enter the name of image...\n",
        "image = cv2.resize(test_image, (32,32))  # our kaggle dataset size is (32,32)\n",
        "image = image.astype(\"float\") / 255.0\n",
        "image = img_to_array(image)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#ret, image1 = cv2.threshold(image, 120, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = np.expand_dims(image, axis=3)\n",
        "model = tf.keras.models.load_model(\"HindiModel2.h5\")  # directory of our model in my PC.\n",
        "results = model.predict(image)[0]  ## only one letter per image...\n",
        "print(labels[np.argmax(results)])\n",
        "\t"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}